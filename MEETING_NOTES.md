- 13.10.21
    - Wie weit weg, wieviel Grad nimmt ein Pixel ein?
        - DVA Pixel to angle conversion
    - Mögliche Kombination RIM/Neural Processes → Trajektorien
    - Problemstellung: Wie kriegt man Stochastizität in RIMs?
    - Google Doc zum Sammeln von Fragen
    - IM - Objekt/Aufgaben
    - Für Feature Pre-processing möglicherweise pretrained ResNet
    - Mögliche Output-Formate: direkt Verteilung, Grids, konkreter Blickpunkt
- 20.10.21
    - Nico macht vielleicht
        - Eye-movement classification
        - Script zum Laden von eye-tracking Ergebnissen
    - Aufbauen auf vorhandenem Code macht Sinn
    - Erstes Starten mit Code-Implementierung
        - Dataloader
            - Kombination Video-Daten mit Eye-Tracking, und EM-Klassifikation
                - EM-Klassifikation von [https://michaeldorr.de/smoothpursuit/](https://michaeldorr.de/smoothpursuit/)
        - naives RIM
    - Beim Kopieren von fremden Repositories Anmerken in erster Zeile
    - Mögliche Teilarbeit: Verschiedene Datensätze standardisieren
        - PySaliency (Tübingen, Matthias)
    - Visual Angle vielleicht nicht sinnvoll?
        - Approximation
        - Saliency Maps auf Pixeln
    - Pytorch Lightning
        - Wrapper mit Parallelisierung
    - Nutzen von Issue-Funktionen in Github
        - Label "help wanted" für Nico/Heiner
    - Heiner: Ruhig mit kleinerem Datenset anfangen, um zu gucken ob das Model funktioniert
- 22.10.21 (Nico Group Talk)
    - NN-saliency: Molin et al 2015, Chang et al 2021
    - Scanpath static: Tatler et al 2017, Schwetlick et al 2020
    - Scanpath in dynamic scenes: Huang et al (2018)
    - End-to-End scanpath → my thesis
    - Nico: Not end-to-end
    - Bottom-up saliency (White et al 2019)
    - One of the most influential scanpath models of all time (Itti el at 1998)
        - saliency + inhibition of return
    - Visual sensitivity (Falk et al 1986)
    - Object-based sensitivity (Egly et al 1994)
    - Object-based selection (Nuthmann & Henderson 2010)
        - not centered around most salient point, but centered around center of object
    - Central fixation bias (Tatler 2007)
        - observers tend to look at center of screen
    - Needed to discard videos where object tracking is not reliable
    - record eye tracking for UVO dataset (object masks given)
        - Test ObjectDDM
    - PixelDDM
        - Inhibition around previously fixated point
    - decomposition of scanpaths Linka & de Haas 2021
        - detections
        - inspections
        - revisits
        - background
        
        → could be checked if there are neural differences in fMRI/EEG
        
    - neurolib (Cakan et al 2021)
        - Python framework for brain modeling
    - Database construction with full control over scenes & tasks
        - Miniature railway scene
- 27.10.21
    - Bumblebee-Video kürzer (und "blöder")
        - vielleicht rausschmeißen
    - Observer rausgeben (beim Dataset)
    - Frames + EM → gaze
    - Frames → gaze + EM (**Ziel**)
    - Frames + gaze → EM
    - Visualisierungen erstellen
        - Frames + raw gaze data (mit EM-labels)
        - Frames + avg gaze data (mit EM-labels)
    - Majority-vote
        - Saccades sehr kurz → vllt immer priorisieren
- 03.11.21
    - Dokumente teilen (Github issues, Google Doc, Notion Markdown exportieren)
    - Im Repository auch Zwischenstände einchecken
    - Averagen für Augenbewegungen problematisch?
        - für Kurven problematisch
        - Abstand zu mean berechnen → Große Abstände?
    - Nico: Lieber Hz beim Averagen nehmen - da Eye-Tracker möglicherweise länger läuft
    - png anstatt jpg nutzen wegen Kompression
    - Visualisierung von averages und Labels priorisieren
    - GazeCom Framerate überprüfen (Bei Yannic verschieden angezeigt)
    - ResNets nicht als Teil des Models, sondern Features vorher abspeichern
        - Features aus verschiedenen Layern nehmen
        - SqueezeNet?
        - Layers aus Anfang, Mitte, Ende
        - 2D, nicht 3D → temporale Komponente den RIMs überlassen
            - Bei 3D müssten Windows definiert werden und sichergestellt werden, dass
        - Räumliche Komponente muss erhalten bleiben
    - Server-Zugriff zum Trainieren klären
    - Embedding zwischenschalten falls Dimensionen zu groß werden
    - Plan für nächstes Meeting
        - Visualisierung von Averaging
        - Pre-processing von Features in ResNets planen
            - Auf Server ausführen?
        - Überlegen, wie Notizen/TODOs am besten geteilt werden können
- 10.11.21
    - Visualisierung von Labeldaten besprochen
    - Frames down-skalieren von 720x1280 (GazeCom)
    - Zu diskutieren: Nur letzten Output von Feature Pyramid oder Stack von Outputs verwenden?
    - Eye-tracking Daten für GazeCom stimmen nicht immer mit Video-Länge überein
        - koenigsstrasse/bumblebee keine 597 Frames
        - YFK_puppies.arff: Daten für 93 Frames fehlen, bei 97 anderen fehlen 1-5 Frames
            - Frage: Wurde Eye-Tracker zu spät angestellt oder zu früh abgeschaltet?
        - Lösung: Als Noise klassifizieren und in der Loss-Function verwerfen
    - Challenge: Loss-Funktion die dazu sorgt dass predictions nicht immer in der Mitte liegen
    - Nächster Schritt: Implementierung feature extraction
        - Feature-Pyramide mit RIM zunächst zusammenlassen, inference time testen
    - Server-Zugriff testen
- 10.11.21
    - Visualisierung von Labeldaten besprochen
    - Frames down-skalieren von 720x1280 (GazeCom)
    - Zu diskutieren: Nur letzten Output von Feature Pyramid oder Stack von Outputs verwenden?
    - Eye-tracking Daten für GazeCom stimmen nicht immer mit Video-Länge überein
        - koenigsstrasse/bumblebee keine 597 Frames
        - YFK_puppies.arff: Daten für 93 Frames fehlen, bei 97 anderen fehlen 1-5 Frames
            - Frage: Wurde Eye-Tracker zu spät angestellt oder zu früh abgeschaltet?
        - Lösung: Als Noise klassifizieren und in der Loss-Function verwerfen
    - Challenge: Loss-Funktion die dazu sorgt dass predictions nicht immer in der Mitte liegen
    - Nächster Schritt: Implementierung feature extraction
        - Feature-Pyramide mit RIM zunächst zusammenlassen, inference time testen
    - Server-Zugriff testen
- 17.11.21
    - 640x360px vielleicht noch zu groß
        - nach ImageNet orientieren (224x224)
            - könnte x gegen y angleichen, quasi normieren
            - menschliche Augenbewegungen vermutlich mehr auf der x-Achse
    - Stochastizität & Funktionssuche für später
    - Parallelisierung des Dataloaders überprüfen
        - Liegt möglicherweise an Windows und kein Problem auf Linux-Server
            - ssh-Verbindung einrichten (mit ProxyJump über alioth)
            - Conda + cuda einrichten
                - Cuda sagen nur eine GPU zu nutzen (am besten im Code)
                - Cuda Toolkit abgleichen mit Server-Version
    - Aggregation von RIM Outputs
        - Im Paper Dense-Layer, aber vermutlich besser ein Attention-Layer anzuhängen
    - Loss-Function
        - exclude noise phases from loss calculations
        - cross-entropy loss
            - möglicherweise weighted, damit Saccaden nicht übergangen werden
        - Zuerst Model von reinen Bilddaten auf gaze + EM-Phase predicten
        - Möglicherweise **Mean Squared Log-scaled Error Loss** benutzen ([https://stats.stackexchange.com/questions/261704/training-a-neural-network-for-regression-always-predicts-the-mean](https://stats.stackexchange.com/questions/261704/training-a-neural-network-for-regression-always-predicts-the-mean))
- 24.11.21
    - Normalisierung überprüfen
        - Visualisierung möglicherweise ohne Normalisierung
    - Kein Einfluss von Bilddaten in Predictions erkennbar
        - FPN gibt möglicherweise Noise aus
            - FPN Features plotten um zu testen, ob Bild-Features und Bewegungen ersichtlich sind
        - Overfitting auf einem bestimmtem Clip um Funktionsweise zu testen
            - Zunächst nur Loss minimieren
        - Rekurrenz entfernen um Einfluss zu testen (nur einen Frame)
    - Irgendwann teacher forcing implementieren
    - Beam search anstatt greedy first approach → längere, schönere Trajectories und weniger Verlass auf
    - Dataloader möglicherweise anpassen um State von letztem Clip zu laden
        - hidden states RIM
        - xy für teacher forcing
            - output als input (Label bzw. vorherige Prediction)
    - Gradient/Weights Update loggen
    - Feature Pyramid Features plotten
        - Tensor in Tensorboard loggen?
- 01.12.21
    - Nur für einen (zwei) Frame trainieren
        - langsam an mehr Frames rantasten
    - Normalisierung für Labels implementieren, Mitte als (0, 0) wählen
    - Loss ist sehr hoch - Sollte überprüft werden
    - Ausprobieren, die Top-Down Parameter einmal aus der Optmierung rausnehmen
    - RIM-weights sollten ebenfalls geloggt werden
    - Für RIM sind 3 Layer sehr viel, am besten am RIM-Paper ausrichten
    - Aggregation zu 2 Gaze Koordinaten überarbeiten
        - Am besten auch an Paper ausrichten
    - Falls Maßnahmen keine Verbesserung ergeben, einfach direkt auf Pixel-Werten lernen (ohne FPN)
- 08.12.21
    - Fortschritts-Update
        - 2s Clip wird (mit etwas jitter) ganz gut gefittet
            - Probleme waren zu kleine Anzahl hidden states in RIM und fehlende Normalisierung der Labels
    - Teacher forcing implementieren
        - An Feature Vektor ground truth von Vorschritt anhängen
        - Vielleicht gleich mehrfach ground truth anhängen, damit sie benutzt wird
        - Mit gewisser wahrscheinlichkeit jeweils teacher forcing anwenden in batch
    - Struktur einbeziehen
        - ConvLSTM
            - Attention und 2D-hidden states
                - Matrix-Mask lernen, Filterbänke anstatt
            - Problematisch für Teacher Forcing
        - Positional Encoding
            - Parameter für Detailliertheit einbauen
    - Parameter-Anzahl kein Problem solange es in den Speicher passt
    - Batchsize sollte idealerweise mindestens 32 sein
        - Sonst unterteilen (Pytorch Lightning hat vielleicht eine Option hierfür)
    - RIM/FPN Parameter Histogramme verändern sich kaum
        - ab jetzt auch Attention loggen
        - Gradient magnitude loggen
            - Veränderung der Gewichte loggen (Falls Update nicht funktioniert)
        - Geschieht eine Normalisierung im Batch Update?
    - Möglicherweise Ansatz mit Feature Pyramid Network nicht gut genug
        - Recherche für Feature Extractor für Videos
- 15.12.21
    - Fortschritt-Update:
        - Teacher-Forcing implementiert, aber loss wird immer noch nicht 0
        - Statistiken über Blickverhalten über Videos/Observers erstellt
    - Bislang mit Nullen gepaddet, wenn Teacher Forcing nicht angewandt wurde
        - Sollte stattdessen Attention output verwenden
        - RIM+Attention für jeden timestep anwenden, hidden states speichern und weitergeben
            - Möglicherweise while loop optimieren mit torchscript
    - Alternative Feature Extractor
        - anderes ResNet model
        - RLVC Autoencoder
        - Pytorchvideo
        - CompressAI
        - Pytorch SlowFast
            - Action recognition
        - Implicit neural representations für Video Daten
- 05.01.22
    - Fortschritt-Update
        - Teacher Forcing angepasst, verwendet jetzt standardmäßig letzten Output
        - Anstatt MobileNet FPN mit EfficientNet B0 versucht
            - schlechtere Performance, und Model/Passes werden riesig
        - Gradient geloggt, gestaffelte Histogramme
            - Nur attention verändert sich, nicht RIM/FPN parameters
            - Gradient verändert sich für weights nicht besonders → vanishing gradient?
        - In RLVC Autoencoder eingearbeitet
    - Scheinbar Outlier bei Gradienten von RIM
        - Möglicherweise Gradient clipping anwenden → Einlesen
    - LSTM weights orthogonal initialisieren
    - Initialisieren und nur Multihead-Attention out-Projection lernen
        - dasselbe mit RIM/FPN
    - Tatsächlichen Gradienten / Learningrate / Varianz loggen
        - Zunächst vielleicht SGD verwenden (zunächst ohne Momentum)
    - RIM-Gewichte normal-verteilt initialisieren?
    - Bias verändert sich, die weights kaum (obwohl Gradienten ungleich null sind)
        - Lediglich FPN Bias scheint sich rege zu verändern
    - Pytorch Cockpit verwenden zum Debuggen
    - In die Zukunft gucken nicht so fatal bei der Feature Extraction
        - 3D Convolution kommt auch in Frage
    - In FPN nur jedes 2. Layer entnehmen, um die Anzahl der Feature kleiner zu halten
    - Video-Autoencoder suchen, von welchem wir Latents entnehmen können
- 19.01.22
    - Fortschritt-Update
        - Einmal nur output attention trainiert
            - → lernt nichts
            - Zusammen mit RIM aber fast normale Ergebnisse (haben aber auch meiste Parameter, FPN ist verhältnismäßig klein)
        - Normal-verteilte Initialisierung (und orthogonal für LSTM) versucht
            - Model lernt plötzlich nicht mehr (warum?)
        - FPN betrachtet nur jedes zweite Layer
            - reduziert Parameter in FPN, allerdings nicht Features, wenn wir nur letztes Layer übernehmen
        - Gradient clipping angewandt, durch Norm und Value
    - RIM durch LSTM ersetzen um zu gucken ob sich weights verändern
        - Sonst RIM mit weniger Parametern
    - Vielleicht irgend ein dummer Fehler im Code (Loss-Funktion)
    - weight decay einmal aussetzen
        - Adam w hat automatisches weight decay
    - Google: Ist es normal dass man in Tensorboard keine Veränderung (Histogramme) sieht?
    - SGD ohne Momentum ausprobieren, besser nachzuvollziehen
    - Aktivierung loggen in Tensorboard
    - Idee: Saliency-maps pro Video aufbauen und viel betrachtete Regionen strikter werten in Loss-Funktion
- 27.01.22
    - Fortschritt-Update
        - Pytorch LSTM anstatt RIM verwendet (multihead-attention + LSTMCell)
            - trainiert deutlich solider und besser
        - Ausprobiert mit n=1 und k=1
            - ähnliche Ergebnisse
            - Probleme anscheinend wenn nicht alle RIM units aktiv sind (k<n)
        - 1 Video + 1 Observer anstatt von 1 Clip + 1 Observer versucht
            - funktioniert leider noch kaum, loss springt sehr und konvergiert nur gering
            - möglicherweise wegen batch_size (train_batch_size = 1)
                - Dataloader umbauen?
    - Unterschied ähnlich wie bei Dropout wenn nicht alle RIMs aktiv sind (nur dass hier in Validation Dropout auch passiert)
    - Bouncing Ball environment hat deutlich kleinere Attention-Dimensionalität
        - Wie können RIMs Ball folgen wenn sie spatial begrenzt sind?
        - Attention über Feature Channels implementieren, nicht alle Pixel
            - key_w,  value_w block-mäßig strukturieren
            - Features per channel embedden
    - Einfach pre-trained Output direkt ohne FPN verwenden
        - oder mit Segmentierungs-Maske (haben wir nicht für GazeCom)
    - Möglicherweise erst nur mit allen RIMs aktiv trainieren (n=k)
    - Weight-changes loggen
- 02.02.22
    - Fortschritts-Update
        - nicht sicher, ob channel-spezifische Attention die Lösung ist, da es im LSTM ja ohne funktioniert
        - RIMs bei k<n schlechter war zu erwarten, aber nicht in diesem Maße - möglicherweise mehr Generalisierung
        - Input-attention channel-wise implementiert
            - RIM-dimension auf HxW gesetzt
            - top_k nach höchster Summe über channel-attention scores
            - alternativ nach niedrigstem score auf Null-Input
        - Schlechtere Ergebnisse als mit voriger Attention
            - Vielleicht findet die Attention bessere Zusammenhänge als die Channel?
        - Habe erstmal nichts gefunden, um die Parameter-Changes aus dem Optimizer zu loggen, aber gucke da nochmal drauf
    - Channel-spezifische Key/Value
        
        → bisher erhalten alle Channels dieselben queries/keys
        
    - Problematisch channels zu mixen in Attention-Prozess
    - Multihead-Attention mit Softmax implementieren
    - RIM-Aktivierung möglicherweise visualisieren
    - Optimizer-Paramete aus model state_dict loggen
- 09.02.22
    - Fortschritts-Update
        - Multihead-Attention with Softmax for channel-specific attention does not perform better than naive attention
        - Tried different key, value, query sizes for attention process, but little impact
    - Param changes stichprobenmäßig plotten
    - Adam Varianz loggen
    - Initialisierung im Originalpaper testen
        - Input-Attention auch uniform initialisiert?
    - Currently blocking gradient through inactive units, but does this make sense?
        - Blocked-gradient im Original-Paper nachvollziehen
    - Mask also on communication attention? Im paper auch auf nicht-aktiven Units
        - vermutlich so dass nur aktive lesen können, aber dann von allen anderen Units
    - Mehr RIMs und auf ganzem Video anwenden