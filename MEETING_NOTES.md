- 13.10.21
    - Wie weit weg, wieviel Grad nimmt ein Pixel ein?
        - DVA Pixel to angle conversion
    - Mögliche Kombination RIM/Neural Processes → Trajektorien
    - Problemstellung: Wie kriegt man Stochastizität in RIMs?
    - Google Doc zum Sammeln von Fragen
    - IM - Objekt/Aufgaben
    - Für Feature Pre-processing möglicherweise pretrained ResNet
    - Mögliche Output-Formate: direkt Verteilung, Grids, konkreter Blickpunkt
- 20.10.21
    - Nico macht vielleicht
        - Eye-movement classification
        - Script zum Laden von eye-tracking Ergebnissen
    - Aufbauen auf vorhandenem Code macht Sinn
    - Erstes Starten mit Code-Implementierung
        - Dataloader
            - Kombination Video-Daten mit Eye-Tracking, und EM-Klassifikation
                - EM-Klassifikation von [https://michaeldorr.de/smoothpursuit/](https://michaeldorr.de/smoothpursuit/)
        - naives RIM
    - Beim Kopieren von fremden Repositories Anmerken in erster Zeile
    - Mögliche Teilarbeit: Verschiedene Datensätze standardisieren
        - PySaliency (Tübingen, Matthias)
    - Visual Angle vielleicht nicht sinnvoll?
        - Approximation
        - Saliency Maps auf Pixeln
    - Pytorch Lightning
        - Wrapper mit Parallelisierung
    - Nutzen von Issue-Funktionen in Github
        - Label "help wanted" für Nico/Heiner
    - Heiner: Ruhig mit kleinerem Datenset anfangen, um zu gucken ob das Model funktioniert
- 22.10.21 (Nico Group Talk)
    - NN-saliency: Molin et al 2015, Chang et al 2021
    - Scanpath static: Tatler et al 2017, Schwetlick et al 2020
    - Scanpath in dynamic scenes: Huang et al (2018)
    - End-to-End scanpath → my thesis
    - Nico: Not end-to-end
    - Bottom-up saliency (White et al 2019)
    - One of the most influential scanpath models of all time (Itti el at 1998)
        - saliency + inhibition of return
    - Visual sensitivity (Falk et al 1986)
    - Object-based sensitivity (Egly et al 1994)
    - Object-based selection (Nuthmann & Henderson 2010)
        - not centered around most salient point, but centered around center of object
    - Central fixation bias (Tatler 2007)
        - observers tend to look at center of screen
    - Needed to discard videos where object tracking is not reliable
    - record eye tracking for UVO dataset (object masks given)
        - Test ObjectDDM
    - PixelDDM
        - Inhibition around previously fixated point
    - decomposition of scanpaths Linka & de Haas 2021
        - detections
        - inspections
        - revisits
        - background
        
        → could be checked if there are neural differences in fMRI/EEG
        
    - neurolib (Cakan et al 2021)
        - Python framework for brain modeling
    - Database construction with full control over scenes & tasks
        - Miniature railway scene
- 27.10.21
    - Bumblebee-Video kürzer (und "blöder")
        - vielleicht rausschmeißen
    - Observer rausgeben (beim Dataset)
    - Frames + EM → gaze
    - Frames → gaze + EM (**Ziel**)
    - Frames + gaze → EM
    - Visualisierungen erstellen
        - Frames + raw gaze data (mit EM-labels)
        - Frames + avg gaze data (mit EM-labels)
    - Majority-vote
        - Saccades sehr kurz → vllt immer priorisieren
- 03.11.21
    - Dokumente teilen (Github issues, Google Doc, Notion Markdown exportieren)
    - Im Repository auch Zwischenstände einchecken
    - Averagen für Augenbewegungen problematisch?
        - für Kurven problematisch
        - Abstand zu mean berechnen → Große Abstände?
    - Nico: Lieber Hz beim Averagen nehmen - da Eye-Tracker möglicherweise länger läuft
    - png anstatt jpg nutzen wegen Kompression
    - Visualisierung von averages und Labels priorisieren
    - GazeCom Framerate überprüfen (Bei Yannic verschieden angezeigt)
    - ResNets nicht als Teil des Models, sondern Features vorher abspeichern
        - Features aus verschiedenen Layern nehmen
        - SqueezeNet?
        - Layers aus Anfang, Mitte, Ende
        - 2D, nicht 3D → temporale Komponente den RIMs überlassen
            - Bei 3D müssten Windows definiert werden und sichergestellt werden, dass
        - Räumliche Komponente muss erhalten bleiben
    - Server-Zugriff zum Trainieren klären
    - Embedding zwischenschalten falls Dimensionen zu groß werden
    - Plan für nächstes Meeting
        - Visualisierung von Averaging
        - Pre-processing von Features in ResNets planen
            - Auf Server ausführen?
        - Überlegen, wie Notizen/TODOs am besten geteilt werden können
- 10.11.21
    - Visualisierung von Labeldaten besprochen
    - Frames down-skalieren von 720x1280 (GazeCom)
    - Zu diskutieren: Nur letzten Output von Feature Pyramid oder Stack von Outputs verwenden?
    - Eye-tracking Daten für GazeCom stimmen nicht immer mit Video-Länge überein
        - koenigsstrasse/bumblebee keine 597 Frames
        - YFK_puppies.arff: Daten für 93 Frames fehlen, bei 97 anderen fehlen 1-5 Frames
            - Frage: Wurde Eye-Tracker zu spät angestellt oder zu früh abgeschaltet?
        - Lösung: Als Noise klassifizieren und in der Loss-Function verwerfen
    - Challenge: Loss-Funktion die dazu sorgt dass predictions nicht immer in der Mitte liegen
    - Nächster Schritt: Implementierung feature extraction
        - Feature-Pyramide mit RIM zunächst zusammenlassen, inference time testen
    - Server-Zugriff testen
- 10.11.21
    - Visualisierung von Labeldaten besprochen
    - Frames down-skalieren von 720x1280 (GazeCom)
    - Zu diskutieren: Nur letzten Output von Feature Pyramid oder Stack von Outputs verwenden?
    - Eye-tracking Daten für GazeCom stimmen nicht immer mit Video-Länge überein
        - koenigsstrasse/bumblebee keine 597 Frames
        - YFK_puppies.arff: Daten für 93 Frames fehlen, bei 97 anderen fehlen 1-5 Frames
            - Frage: Wurde Eye-Tracker zu spät angestellt oder zu früh abgeschaltet?
        - Lösung: Als Noise klassifizieren und in der Loss-Function verwerfen
    - Challenge: Loss-Funktion die dazu sorgt dass predictions nicht immer in der Mitte liegen
    - Nächster Schritt: Implementierung feature extraction
        - Feature-Pyramide mit RIM zunächst zusammenlassen, inference time testen
    - Server-Zugriff testen
- 17.11.21
    - 640x360px vielleicht noch zu groß
        - nach ImageNet orientieren (224x224)
            - könnte x gegen y angleichen, quasi normieren
            - menschliche Augenbewegungen vermutlich mehr auf der x-Achse
    - Stochastizität & Funktionssuche für später
    - Parallelisierung des Dataloaders überprüfen
        - Liegt möglicherweise an Windows und kein Problem auf Linux-Server
            - ssh-Verbindung einrichten (mit ProxyJump über alioth)
            - Conda + cuda einrichten
                - Cuda sagen nur eine GPU zu nutzen (am besten im Code)
                - Cuda Toolkit abgleichen mit Server-Version
    - Aggregation von RIM Outputs
        - Im Paper Dense-Layer, aber vermutlich besser ein Attention-Layer anzuhängen
    - Loss-Function
        - exclude noise phases from loss calculations
        - cross-entropy loss
            - möglicherweise weighted, damit Saccaden nicht übergangen werden
        - Zuerst Model von reinen Bilddaten auf gaze + EM-Phase predicten
        - Möglicherweise **Mean Squared Log-scaled Error Loss** benutzen ([https://stats.stackexchange.com/questions/261704/training-a-neural-network-for-regression-always-predicts-the-mean](https://stats.stackexchange.com/questions/261704/training-a-neural-network-for-regression-always-predicts-the-mean))
- 24.11.21
    - Normalisierung überprüfen
        - Visualisierung möglicherweise ohne Normalisierung
    - Kein Einfluss von Bilddaten in Predictions erkennbar
        - FPN gibt möglicherweise Noise aus
            - FPN Features plotten um zu testen, ob Bild-Features und Bewegungen ersichtlich sind
        - Overfitting auf einem bestimmtem Clip um Funktionsweise zu testen
            - Zunächst nur Loss minimieren
        - Rekurrenz entfernen um Einfluss zu testen (nur einen Frame)
    - Irgendwann teacher forcing implementieren
    - Beam search anstatt greedy first approach → längere, schönere Trajectories und weniger Verlass auf
    - Dataloader möglicherweise anpassen um State von letztem Clip zu laden
        - hidden states RIM
        - xy für teacher forcing
            - output als input (Label bzw. vorherige Prediction)
    - Gradient/Weights Update loggen
    - Feature Pyramid Features plotten
        - Tensor in Tensorboard loggen?
- 01.12.21
    - Nur für einen (zwei) Frame trainieren
        - langsam an mehr Frames rantasten
    - Normalisierung für Labels implementieren, Mitte als (0, 0) wählen
    - Loss ist sehr hoch - Sollte überprüft werden
    - Ausprobieren, die Top-Down Parameter einmal aus der Optmierung rausnehmen
    - RIM-weights sollten ebenfalls geloggt werden
    - Für RIM sind 3 Layer sehr viel, am besten am RIM-Paper ausrichten
    - Aggregation zu 2 Gaze Koordinaten überarbeiten
        - Am besten auch an Paper ausrichten
    - Falls Maßnahmen keine Verbesserung ergeben, einfach direkt auf Pixel-Werten lernen (ohne FPN)
- 08.12.21
    - Fortschritts-Update
        - 2s Clip wird (mit etwas jitter) ganz gut gefittet
            - Probleme waren zu kleine Anzahl hidden states in RIM und fehlende Normalisierung der Labels
    - Teacher forcing implementieren
        - An Feature Vektor ground truth von Vorschritt anhängen
        - Vielleicht gleich mehrfach ground truth anhängen, damit sie benutzt wird
        - Mit gewisser wahrscheinlichkeit jeweils teacher forcing anwenden in batch
    - Struktur einbeziehen
        - ConvLSTM
            - Attention und 2D-hidden states
                - Matrix-Mask lernen, Filterbänke anstatt
            - Problematisch für Teacher Forcing
        - Positional Encoding
            - Parameter für Detailliertheit einbauen
    - Parameter-Anzahl kein Problem solange es in den Speicher passt
    - Batchsize sollte idealerweise mindestens 32 sein
        - Sonst unterteilen (Pytorch Lightning hat vielleicht eine Option hierfür)
    - RIM/FPN Parameter Histogramme verändern sich kaum
        - ab jetzt auch Attention loggen
        - Gradient magnitude loggen
            - Veränderung der Gewichte loggen (Falls Update nicht funktioniert)
        - Geschieht eine Normalisierung im Batch Update?
    - Möglicherweise Ansatz mit Feature Pyramid Network nicht gut genug
        - Recherche für Feature Extractor für Videos
- 15.12.21
    - Fortschritt-Update:
        - Teacher-Forcing implementiert, aber loss wird immer noch nicht 0
        - Statistiken über Blickverhalten über Videos/Observers erstellt
    - Bislang mit Nullen gepaddet, wenn Teacher Forcing nicht angewandt wurde
        - Sollte stattdessen Attention output verwenden
        - RIM+Attention für jeden timestep anwenden, hidden states speichern und weitergeben
            - Möglicherweise while loop optimieren mit torchscript
    - Alternative Feature Extractor
        - anderes ResNet model
        - RLVC Autoencoder
        - Pytorchvideo
        - CompressAI
        - Pytorch SlowFast
            - Action recognition
        - Implicit neural representations für Video Daten
- 05.01.22
    - Fortschritt-Update
        - Teacher Forcing angepasst, verwendet jetzt standardmäßig letzten Output
        - Anstatt MobileNet FPN mit EfficientNet B0 versucht
            - schlechtere Performance, und Model/Passes werden riesig
        - Gradient geloggt, gestaffelte Histogramme
            - Nur attention verändert sich, nicht RIM/FPN parameters
            - Gradient verändert sich für weights nicht besonders → vanishing gradient?
        - In RLVC Autoencoder eingearbeitet
    - Scheinbar Outlier bei Gradienten von RIM
        - Möglicherweise Gradient clipping anwenden → Einlesen
    - LSTM weights orthogonal initialisieren
    - Initialisieren und nur Multihead-Attention out-Projection lernen
        - dasselbe mit RIM/FPN
    - Tatsächlichen Gradienten / Learningrate / Varianz loggen
        - Zunächst vielleicht SGD verwenden (zunächst ohne Momentum)
    - RIM-Gewichte normal-verteilt initialisieren?
    - Bias verändert sich, die weights kaum (obwohl Gradienten ungleich null sind)
        - Lediglich FPN Bias scheint sich rege zu verändern
    - Pytorch Cockpit verwenden zum Debuggen
    - In die Zukunft gucken nicht so fatal bei der Feature Extraction
        - 3D Convolution kommt auch in Frage
    - In FPN nur jedes 2. Layer entnehmen, um die Anzahl der Feature kleiner zu halten
    - Video-Autoencoder suchen, von welchem wir Latents entnehmen können
- 19.01.22
    - Fortschritt-Update
        - Einmal nur output attention trainiert
            - → lernt nichts
            - Zusammen mit RIM aber fast normale Ergebnisse (haben aber auch meiste Parameter, FPN ist verhältnismäßig klein)
        - Normal-verteilte Initialisierung (und orthogonal für LSTM) versucht
            - Model lernt plötzlich nicht mehr (warum?)
        - FPN betrachtet nur jedes zweite Layer
            - reduziert Parameter in FPN, allerdings nicht Features, wenn wir nur letztes Layer übernehmen
        - Gradient clipping angewandt, durch Norm und Value
    - RIM durch LSTM ersetzen um zu gucken ob sich weights verändern
        - Sonst RIM mit weniger Parametern
    - Vielleicht irgend ein dummer Fehler im Code (Loss-Funktion)
    - weight decay einmal aussetzen
        - Adam w hat automatisches weight decay
    - Google: Ist es normal dass man in Tensorboard keine Veränderung (Histogramme) sieht?
    - SGD ohne Momentum ausprobieren, besser nachzuvollziehen
    - Aktivierung loggen in Tensorboard
    - Idee: Saliency-maps pro Video aufbauen und viel betrachtete Regionen strikter werten in Loss-Funktion
- 27.01.22
    - Fortschritt-Update
        - Pytorch LSTM anstatt RIM verwendet (multihead-attention + LSTMCell)
            - trainiert deutlich solider und besser
        - Ausprobiert mit n=1 und k=1
            - ähnliche Ergebnisse
            - Probleme anscheinend wenn nicht alle RIM units aktiv sind (k<n)
        - 1 Video + 1 Observer anstatt von 1 Clip + 1 Observer versucht
            - funktioniert leider noch kaum, loss springt sehr und konvergiert nur gering
            - möglicherweise wegen batch_size (train_batch_size = 1)
                - Dataloader umbauen?
    - Unterschied ähnlich wie bei Dropout wenn nicht alle RIMs aktiv sind (nur dass hier in Validation Dropout auch passiert)
    - Bouncing Ball environment hat deutlich kleinere Attention-Dimensionalität
        - Wie können RIMs Ball folgen wenn sie spatial begrenzt sind?
        - Attention über Feature Channels implementieren, nicht alle Pixel
            - key_w,  value_w block-mäßig strukturieren
            - Features per channel embedden
    - Einfach pre-trained Output direkt ohne FPN verwenden
        - oder mit Segmentierungs-Maske (haben wir nicht für GazeCom)
    - Möglicherweise erst nur mit allen RIMs aktiv trainieren (n=k)
    - Weight-changes loggen
- 02.02.22
    - Fortschritts-Update
        - nicht sicher, ob channel-spezifische Attention die Lösung ist, da es im LSTM ja ohne funktioniert
        - RIMs bei k<n schlechter war zu erwarten, aber nicht in diesem Maße - möglicherweise mehr Generalisierung
        - Input-attention channel-wise implementiert
            - RIM-dimension auf HxW gesetzt
            - top_k nach höchster Summe über channel-attention scores
            - alternativ nach niedrigstem score auf Null-Input
        - Schlechtere Ergebnisse als mit voriger Attention
            - Vielleicht findet die Attention bessere Zusammenhänge als die Channel?
        - Habe erstmal nichts gefunden, um die Parameter-Changes aus dem Optimizer zu loggen, aber gucke da nochmal drauf
    - Channel-spezifische Key/Value
        
        → bisher erhalten alle Channels dieselben queries/keys
        
    - Problematisch channels zu mixen in Attention-Prozess
    - Multihead-Attention mit Softmax implementieren
    - RIM-Aktivierung möglicherweise visualisieren
    - Optimizer-Paramete aus model state_dict loggen
- 09.02.22
    - Fortschritts-Update
        - Multihead-Attention with Softmax for channel-specific attention does not perform better than naive attention
        - Tried different key, value, query sizes for attention process, but little impact
    - Param changes stichprobenmäßig plotten
    - Adam Varianz loggen
    - Initialisierung im Originalpaper testen
        - Input-Attention auch uniform initialisiert?
    - Currently blocking gradient through inactive units, but does this make sense?
        - Blocked-gradient im Original-Paper nachvollziehen
    - Mask also on communication attention? Im paper auch auf nicht-aktiven Units
        - vermutlich so dass nur aktive lesen können, aber dann von allen anderen Units
    - Mehr RIMs und auf ganzem Video anwenden
- 16.02.22
    - Fortschritts-Update
        - Bug in Input-Attention fixed, dass topk vor dem Softmax angewendet wurde
            - Ausprobiert mit einzelnem Clip, einzelnem Video and allen Videos
                - Alle Videos Probleme wegen Logging overhead log overhead
            - Bei Sampling von Model, welches auf einzelnem Video trainiert wurde beginnt die Prediction mit stark versetztem Scanpath, bevor es gut vorhersagt
        - Leichte Fixierung zur Mitte in Videos erkennbar
            - Loss anpassen?
    - Loss spezialisieren
        - Mitte bestrafen
            - Etwas besseres als L2-Loss? (e.g. L1-Loss)
        - spezialisieren auf Saccade/Loss/FP
            - Trajektorien berücksichtigen
                - Teacher forcing für Bewegungsrichtung?
    - Eye-Movement phase Prediction reinnehmen in das Model
        - Idealerweise lernt das Model das phases und gaze korrelieren
        - Möglicherweise müssen gaze loss aus MSE/L1 und EM-loss aus cross_entropy noch verschieden gewichtet werden
    - Möglicherweise RIM-Spezialisierung überprüfen
        - Direkt aus Backbone Features entnehmen (Top-Down Prozess auslassen) und gucken welche Layer verwendet werden
            - Allerdings aufwending und möglicherweise problematisch mit geflatteten RIM-Input
    - Nach Bugfix nochmal Hyperparameter-Suche durchführen, schauen ob wir realistischen Scanpath wenigstens auf einem Video hinkriegen (so dass Nico es nicht unterscheiden kann)
    - Im RIM-Paper wird für strukturierten Input Positional Encoding vorgeschlagen, möglicherweise in Patches anwenden
        - Allerdings aufwendig und Patches nicht flexibel (falls Objekte durch Patches gehen)
- 02.03.22
    - Fortschritts-Update:
        - l1, smooth_l1 loss getestet um Mitt-Fokus entgegen zu wirken
            - Trajektorie wirkt komisch für smooth_l1
            - l1 scheint Objekte besser zu verfolgen, es ist aber noch viel Interpretation dabei
        - EM-Phasen in prediction einbezogen
            - EM cross-entropy loss 4x so groß wie gaze loss
                - Regularization durch weighted cross-entropy
            - Kurze Phasen werden gerne plattgemacht
                - Hier Sakkaden extra gewichten in weighted cross-entropy
                    - trotzdem sind Predictions hier sehr zufällig
        - Direkt mit Features von vorletztem (6) und vor-vorletztem Layer (5) von MobileNet versucht
            - Performt schlechter als FPN, FPN funktioniert also
    - Gaze Regression hat generell Priorität über EM-Klassifikation, also möglicherweise Klassifikation auslassen
    - Mit Gewichtung und EM-Phasen auf kleinem Clip erneut versuchen, ob Phasen + Gaze gut gelernt werden kann
        - Gewichtung in cross-entropy an Anteile auf Videodaten anlegen
    - breite_strasse ist problematisches Trainingsvideo, da zuviel passiert und immer etwas in der Mitte ist
        - Besseres Trainingsvideo: Doves, Duck_boat (highest attention synchrony)
            - Doves aber auch sehr Mitt-lastig
    - Loss anpassen - während Sakkaden Trägheit bestrafen, sonst Ruckartigkeit bestrafen
        - Regularization gegen zittern der Trajektorie
            - L2-Norm ist ok dafür, bei Sakkaden einfach flippen
    - Gradienten vergleichen für Regression/Klassifikation für Regularisierung von Klassifikation-Loss (Paper von Heiner)
        - Möglicherweise Auxillary loss anwenden, zunächst aber einfach über GradNorm gehen
    - Neues RIM-Paper schlägt Global Workspace vor
        - möglicherweise implementieren (Heiner linkt Paper)
    - Zu mehreren Observern übergehen
        - Vielleicht widersprüchliche Observer verwenden, um zu überprüfen ob eine Trajektorie gewählt wird, oder aber nur die Mitte
        - Auf vielen Videos trainieren, damit es eine Chance auf Generalization gibt
    - Metriken auswerten über die Realität von ausgegebenen Trajektorien
        - Nico gibt Paper weiter, welches den Normalized Scanpath als Metrik anbietet
        - Durchschnitt über mehrere Samples wählen
        - Wahrscheinlich Gaussian Kernel Smoothing verwenden
- 09.03.22
    - Fortschritts-Update:
        - Paper zu GradNorm, Normalized Scanpath und Shared Global Workspace gelesen
        - GradNorm implementiert, allerdings verändern sich die Gewichte noch nicht trotz Gradient
    - GradNorm
        - versuche separaten Optimizer mit anderer learning rate, möglicherweise ist die learning rate einfach zu niedrig
    - Normalized Scanpath Saliency
        - Zu visual angle umrechnen
            - Vllt einfach mit Pixel-Conversion, aber sonst mit bestehender Funktion
        - NSS maps für jedes Video erstellen
    - Regularisierung
        - Nicht-Sakkaden L2-Loss, um “Zittern” zu bestrafen
        - Sakkaden negativer L2-Loss, um Stillstand zu bestrafen
        - Regularisierung durch Lambda-Term, der als Hyperparameter mit eingeht
            - GradNorm hier nicht sinnvoll, da dieser Loss nicht gleich wichtig ist
    - Möglicherweise für Conference Workshops anmelden (z.B. Juni)
        - CVPR, ECCV
- 23.03.22
    - Fortschritts-Update:
        - Regularisierung eingebaut
            - Für Fixation und Smooth Pursuit pos. L2-Loss zum Vorgänger
            - Für Sakkade neg. L2-Loss zum Vorgänger
            - Wirkt sehr fahrig für größere Regularisierung
                - Negativer L2-Loss ermutigt möglicherweise größere Sprunghaftigkeit da es nicht genau erkennt wann etwas eine Sakkade ist und wann nicht
        - Normalized Scanpath Saliency angefangen einzubauen
            - Problematik eine variable Funktion mit Summen von Exp von Summen zu bauen, die ich pro Video speichern kann
            - Gaze raw oder pro Frame nehmen?
    - Regularisierung
        - L1-loss für Sakkaden-Regularisierung verwenden um Explosion zu verhindern
        - Effekt von Regularisierung mit Training auf mehreren Clips testen, ob der Mitt-Jitter unterbunden wird
    - Normalized Scanpath Saliency
        - 3D-Kernel Estimator verwenden, pro Video berechnen, picklen und speichern
        - Ruhig ohne Zeitfenster und stattdessen über gesamtes Video betrachten
    - Videos für Training und Validation überdenken
        - Sollte recht eindeutige Saliency besitzen
        - Möglichst nicht zu mitt-lastig
    - Grobe Struktur Masterarbeit entwerfen
        - Arbeit ist sehr technisch, aber noch ein bisschen mehr das große Ganze einbringen
        - Verbindungen zur Psychologie bei technischen Methoden aufzeigen
            - RIM-Spezialisierung, Attention
    - Schauen ob es bei Psychologen ML-Ansatz gibt, und wie diese dabei vorgegangen sind
    - Was sind die shortcomings von einem manuelleren Modell?
        - Performance
        - In Nico’s Erfahrung geht die Frage eher anders herum → warum nicht Deep Learning approach?
        - RIM als Mittelding zwischen Interpretibility und Performance vorstellen
            - Idealerweise RIM-Spezialisierung aufzeigen
    - Heiner: Wichtig dass das Modell auf mehreren Observern funktioniert
        - Bald übergehen, bevor wir bei einem Clip übermäßig tunen und etwas übersehen
- 30.03.22
    - Fortschritts-Update:
        - Auf golf als Hauptvideo umgeschwenkt
            - Validierung auf doves, ducks_children, holsten_gate, puppies
            - breite_strasse ist draußen, holsten_gate noch für Validierung von ganzem Video genutzt
        - Normalized Scanpath Saliency mit KernelDensityEstimator implementiert
            - Gibt allerdings noch keine sinnvollen Werte aus
            - Normalization durch samplen von Werten sehr langsam durch Dimensionalität
        - L1-Loss Sakkaden-Regularisierung implementiert
        - 2 Hyperparameter für Fixation-und Sakkaden-Regularisierung
            - Auf ganzem Video sind Ergebnisse allerdings nicht zufriedenstellend
        - Beginn Arbeit auszuskizzieren
    - Normalized Scanpath Probleme
        - Nico schickt noch einmal Code, möglicherweise naivere Version einbauen
    - Visual Angle doppelter Winkel, oder nur in eine Richtung?
        - Nico hat nicht doppelten verwendet, aber möglicherweise nochmal überprüfen
    - Zappeln der Gaze-Prediction bleibt trotz Regularisierung
        - Vielleicht nur Position-Change predicten, nicht absoluten Wert
    - Regularisierung durch Aktivierungsfunktion
        - Kleine Changes nicht besonders beachten
        - x³, Softplus, tanh oder absoluten Cutoff
    - Übergehen zu mehreren Videos, dann zu mehreren Subjects
        - Testen auf ungesehenen Videos
        - Testen auf ungesehenen Observern
            - Möglicherweise schwierig, da Observer höchst chaotisch sein können
    - Abschnitte der Masterarbeit zum nächsten Mal planen
- 06.04.22
    - Fortschritts-Update:
        - Trainieren auf Gaze change anstatt absoluten Werten
            - atanh → Diff (rückwärts cumsum → tanh) eingebaut um im Wertebereich zu bleiben
                - Mit Aktivierungsfunktion x³ und ohne auf einem Clip trainiert
            - Bisher größtenteils Noise, mit x³ nur sehr kleine Änderungen
        - Nicos Ansatz für NSS eingebaut
            - Scores für jeden Observer+Video genommen
            - Muss nun noch ins Samplen eingebaut werden
        - Einmal auf allen Videos trainiert - ca. 17min pro Epoche
            - Predictions sind wieder sehr mittig
    - Normalized Scanpath Saliency
        - Random baseline nicht sinnig für NSS
            - Baseline in der Mitte besserer Vergleichswert
        - Möglicherweise interessant über Window-Funktion Korrelation in einzelnen Clips zu betrachten
    - Predictions auf allen Videos sehr mittig
        - RIM größer machen für alle Videos
        - Dropout erhöhen, Unterschiede testen
            - Dropout im Test anlassen (Batchnorm eval)
    - Dropout testen, gleiche Videos erneut testen ob sie verschiedenes generieren
    - Dropout wird nach Softmax angewandt, wodurch Normalisierung nicht mehr gegeben ist
        - Einmal umdrehen, Effekt testen
    - Verteilung als möglicher Model-Output
        - Mixture of Gaussians
        - loss über Log-Likelihood bestimmen
    - Flowchart für Modell anfertigen
        - Mit Flags, Hyperparametern
        - Einmal alle Baustellen visualisieren
- 13.04.22
    - Fortschritts-Update:
        - NSS in model sampling eingebaut
            - mit Vergleich zu NSS von originalem Clip und Mitt-NSS
            - NSS für loss benutzen? Hat allerdings Target-Information, müsste in torch implementiert werden und ist computationally expensive
        - Dropout getestet
            - generell aktiv, jedes Mal andere Scanpaths für gleichen Input
        - Auf allen Videos + Observers trainiert mit gaze changes und Aktivierungsfunktion x³
            - Resultiert in mittigem Noise
            - Ich glaube gaze change ist im Moment eine Sackgasse
        - Thesis-Template weiter gestaltet/strukturiert
        - Paper gelesen für grundsätzlichere Ansätze
            - NSS als Loss?
            - Fixation-Klassifikation über Pixel?
            - Anderen Backbone für Feature Extractor?
    - Möglicherweise Mixture of Gaussians als Output (äquivalent zu NSS als Loss)
        - Nico: Mixture of Gaussians erstmal nicht Priorität
        - Gaussian kann problematisch an Bildrändern sein
    - Alle videos + 1 observer einmal ausprobieren
        - Wenn es nicht funktioniert → Feature Extraction nicht gut genug
    - längere Clip-sizes ausprobieren
        - einmal auf ganzen 20s videos trainieren
    - Mehrere predictions auf gleichem Input vergleichen
        - NSS-average berechnen
        - Alle plotten / Scanpaths vergleichen
        - Gaussian smoothing als saliency map
            - im Vergleich mit NSS-Map plotten
    - Nächste Woche ein paar Plots zeigen
    - Scanpath-Output vergleichen zwischen trainiert/untrainiert
    - Möglicherweise Vortrag über den aktuellen Stand für Prof. Obermayer vorbereiten
    - Nico: Wichtig immer wieder Ergebnisse/Probleme zu sammeln
- 20.04.22
    - Fortschritts-Update:
        - Auf allen Videos + 1 Observer trainiert
            - Ähnlicher Loss wie bei 1 Video + 1 Observer, Output orientiert sich allerdings nicht merklich an Features
            - Kein Unterschied für größeres Modell
        - Einmal mit p_teacher_forcing=1 trainiert
            - train_loss wird nicht null
        - Bei >12s kommt Fehler “too many open files” wegen offenen Frames
            - Neuer Dataloader, welcher Videos direkt ins Memory lädt?
        - Mehrere scanpaths pro video clip gesampelt
            - Mean NSS als Metrik
            - Plotten aller Scanpaths
                - Sind sehr nah beieinander → mehr seeding benötigt? (höherer dropout?)
        - Gaussian density von Observer-Daten animiert
    - Samplen von custom Clips in Dataloader einbauen, damit interpretierbare Clips entnommen werden
    - Predictions sind sehr mittig, achten wenig auf Features
        1. Option: Changes vorhersagen
            1. Problem loss - Cumsum loss verwenden
        2. Option: Mehrere Gaussians als Output
            1. p_cluster (Softmax)/mean/std
            2. Problem Sprünge (durch Teacher Forcing/Recurrence?)
            3. Auf allen Observern trainieren
    - Predictions randomisieren nicht vernünftig
        - Probablistischer Output
        - Mixture of Gaussians
    - Teacher Forcing erst nach Input attention anwenden
        - Problematisch für Aktivierung und Dropout
    - Input attention visualisieren
        - Nimmt das Netz Features wahr?
    - Idee Nico: Einzelne RIMs besonders bevorzugt behandeln
        - Bringt möglicherweise Zufälligkeit
        - Eine RIM, welche nur für Sakkaden aktiviert wird
    - Mögliche Anpassung: k aktive RIMs aus attention_probs samplen, anstatt topk zu wählen
        - Vielleicht flexible Anzahl RIMs aktivieren (ziehen mit zurücklegen)
    - Mögliche Anpassung: Softmax temperature parameter einbauen
        - Je höher, desto extremer
    - Scanpaths sind zu Beginn verstreut, konvergieren dann (auch auf untrainiertem Modell)
        - Etwas schnelles einbauen, auf kleinerem Datenset bzw. Änderungen einfach auf untrainiertem Modell ausprobieren
        - Woran liegt dies? Untrainiert sollten Scanpaths unabhängig voneinander sein
    - Saliency Map über Video legen, um zu sehen, worauf Observer achten
- 27.04.22
    - Fortschritts-Update:
        - Overlay von gaussian density über video samples implementiert
        - Sampling von spezifischen Clips implementiert
        - Testen ob untrainierte RIM gegen gemeinsamen Wert (und null) für gleichen Input konvergiert
            - Startpunkt ist random, nach 2-3 Zeitschritten random im Bereich [-0.1, 0.1], wie bei LSTM
        - Teacher Forcing in RIM nach Input Attention eingebaut
            - allerdings keine Verbesserung im Training
        - Lange (200 Epochen) trainiert auf allen Videos/1 observer
            - Funktioniert gut auf Trainingsdaten, nicht wirklich auf Validation-Set
                - Generalisierung funktioniert nicht
            - l1_loss liefert keine guten Ergebnisse
    - RIM ohne Dropout generiert verschiedene Traces
        - hidden state bei Initialisierung setzen, nicht im Forward pass
    - Möglicherweise Positional Encoding als sin-Überlagerung einbauen
        - Nico: Vielleicht erst etwas Stabiles schaffen, bevor neue Baustellen aufgemacht werden
    - Aktive RIMs und Input Attention visualisieren
    - Vorschlag Nico: n=2, k=1 ausprobieren
        - Korrelation Fixation/Sakkade mit RIMs überprüfen
    - Falls Zeit es hergibt, Gaussian Mixture output implementieren, da gaze change prediction nicht vielversprechend scheint
    - Masterarbeit anfangen zu schreiben / Struktur mit Figures grob erstellen
        - Dann können Nico/Heiner rübergucken und Sachen frühzeitig korrigiert werden
    - Prof. Obermayer über die Arbeit rübergucken lassen
        - Kurze Präsentation anfertigen (sehr exakt sein!)
    - Verteidigung möglicherweise am 01.07.22, 2 Wochen nach Abgabe
- 04.05.22
    - Fortschritts-Update:
        - Verschiedene Backbone-Netze eingebaut
            - Ähnlicher loss mit größeren Netzen (z.B. DenseNet, EfficientNet B7)
        - Positional Encoding als lernbaren Parameter eingebaut
            - Liefert allerdings schlechtere Ergebnisse im Training
        - Aktive RIM units können jetzt geloggt werden
        - Auf gaze changes trainiert, aber kein Erfolg
    - Anstatt von Positional Encoding Conv LSTM möglich in RIM
        - Group convolution benutzen
    - Wie kann Input attention gut visualisiert werden?
        - Gradient angucken (1 als input)
    - Clip duration kürzer machen (analog Deepgaze → 4 letzte Sakkaden)
        - Auch auf untrainiertem Netzwerk Einfluss testen
        - Deepgaze sagt nur Sakkaden vorher, können mehrere Frames dazwischen sein
    - Gaze changes predicten
        - Vielleicht ganz ohne arctanh/tanh Transformation
            - Dann diagonal zurücksetzen falls es out-of-bounds ist
        - Erstmal nur auf cumsum-loss trainieren (beinhaltet individuellen Loss bereits)
    - Gaussian mixture einbauen?
        - Aufgrund der verbleibenden Zeit niedrigere Prio, lieber in der Arbeit dann diskutieren
    - Einmal sequentiell, einmal im gleichen Batch gleichen Input testen, könnte Fehler aufzeigen
    - 1 video, all observers trainieren, prüfen ob dort die Problematik ähnlich ist
    - Kurze Präsentation für Prof. Obermayer nächste Woche Donnerstag 14 Uhr
    - Wie kann erster LSTM hidden state gesetzt werden?
        - Nur in erstem RIM layer setzen
        - Nur hidden state, nicht cell state (fließt nicht in Input attention ein)
        1. Trainieren abhängig von observer(, video, clip_start)
        2. Alternativ einfach ID als seed und zufällig generieren
        3. Oder Observer-Information in query einfließen lassen
            1. h_t konkatenieren mit Observer-Embedding, W_Q erweitern
        4. Dropout-Seed als Observer (+ Video, start-frame) setzen
            1. problematisch, da pytorch-seed nur global gesetzt werden kann
    - Vielleicht mehrere RIM-Layer ausprobieren
- 11.05.22
    - Fortschritts-Update:
        - 2 RIM units trainiert, allerdings keine Spezialisierung in Fixation/Sakkade erkennbar
            - RIM activations und eye movement classification werden geloggt
        - Bug in Positional Encoding gefixt, lernt aber keine intuitiven Maps
        - Mit clip duration=0.5s trainiert
            - Loss bleibt sehr hoch → Vermutlich auch wegen h_0=0, hat weniger Zeit den hidden state zu bilden
        - Präsentation für Prof. Obermayer aufgebaut
            - NSS score über 100 random clips berechnet
        - Gradient durch Input attention visualisiert
            - Sieht leider zufällig verteilt aus, keine Objekte erkennbar
                - Verschwindet Gradient vielleicht und wird die Input attention so nicht trainiert?
        - 2 Rim layer ausprobiert → Keine Verbesserung
        - Gaze change prediction funktioniert immer noch gar nicht
            - Möglicherweise Code auf Bugs checken?
    - RIM activations mit n=2 noch einmal auf nur einem Clip overfitten, wo Fixation und Sakkade auch sichtbar sind
        - RIM Aktivierungen und Eye movement Klassifikation in ein Plot packen, damit Korrelationen besser sichtbar sind
    - Wenig Frames  mit Sakkaden, unbalanciertes Datenset
        - Vielleicht loss für Sakkaden höher gewichten
            - Regularisierung kann problematisch sein, da irgenwann die Regularisierung für Sakkaden über MSE zur ground truth überwiegt
            - MSE-loss verschieden gewichten für Sakkade und Fixation/SP
    - NSS scores als Linien für verschiedene Videos plotten
        - Checken ob Differenz einfach zwischen NSS-scores berechnet werden kann (log-likelihood?)
    - Einmal ganze Videos samplen
        - Falls Visualisierung lange dauert, einfach Clips aus Output samplen
    - Input attention
        - Wird Gradient verloren, trotz retain_graph=True
        - Einheitliche Farbskala in Gradient Visualisierung, damit wenigstens Attention über Channel betrachtet werden kann
    - Gibt es andere Metriken für Scanpath, welche Bewegungs-Dynamiken besser betrachten?
        - Wie hat DeepGaze dies ausgewertet?
        - Normalerweise oft über Anteil Sakkade/Fixation, allerdings Klassifikation schwierig solange Bewegungs-Dynamik nicht passt
            - Stattdessen Histogramm von gaze changes / Winkeln betrachten
                - Falls es zu viele kleine Werte gibt, log-scale oder threshholding anwenden